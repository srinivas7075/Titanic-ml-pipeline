ğŸ“˜ End-to-End Machine Learning Pipeline â€” Titanic Survival Prediction

A complete end-to-end Machine Learning Pipeline built using the Titanic dataset.
This project demonstrates the full ML workflow including:

âœ” Data Cleaning
âœ” Exploratory Data Analysis (EDA)
âœ” Feature Engineering
âœ” Model Training
âœ” Model Evaluation
âœ” Final Model Selection
ğŸ“¦ Dataset
-------------------------------------------------------
ğŸ“¦ Dataset

Source: KaggleHub â†’ yasserh/titanic-dataset

File Used: Titanic-Dataset.csv

Rows: 891

Columns: 12

Target variable: Survived (0 = died, 1 = survived)

ğŸ§¹ Data Cleaning
-------------------------------------------------------


Performed the following steps:

âœ” Missing Value Handling

Age â†’ median imputation

Embarked â†’ mode imputation

Cabin â†’ deck extraction â†’ missing â†’ 'U'

âœ” Outlier Treatment

Applied IQR method to Age and Fare.

âœ” Data Type Fixing

Converted Pclass, Sex, Embarked, and Deck to categorical types.

âœ” Duplicate Removal

No duplicates found.

ğŸ“Š Exploratory Data Analysis (EDA)
---------------------------------------------------------


Generated visualizations including:

Age Distribution

Fare Distribution by Class

Survival Counts by Gender

Survival by Embarked Port

Age vs Fare Scatter

Correlation Heatmap

Deck Distribution

ğŸ” Key Insights
---------------------------------------------------------


Female passengers had much higher survival rates than males.

Higher passenger class (1st class) correlated strongly with survival.

Higher fare values associated with higher survival.

Age had a weak negative relationship with survival.

Family-based features such as FamilySize influenced survival patterns.

ğŸ›  Feature Engineering
---------------------------------------------------------


Created multiple high-impact features:

âœ” Encoded Features

Sex_enc (Label Encoding)

One-hot encoding for:

Embarked (Embarked_Q, Embarked_S)

Pclass (Pclass_2, Pclass_3)

Deck (Deck A, B, C, D, E, F, G, T, U)

âœ” New Features

FamilySize

IsAlone

Title extraction from Name (mapped rare titles)

Title_enc

âœ” Scaling

Age_s and Fare_s using StandardScaler

ğŸ¤– Model Training
-------------------------------------------------------

Two models were trained:

1ï¸âƒ£ Logistic Regression

Best performance

Great baseline model

Low variance, stable metrics

2ï¸âƒ£ Random Forest

Good model, but slightly lower performance in this dataset

ğŸ“ˆ Model Evaluation
------------------------------------------------------------
Logistic Regression

Accuracy: 0.8156

Precision: 0.8000

Recall: 0.6957

F1-score: 0.7442

ROC-AUC: 0.8365

Random Forest

Accuracy: 0.7877

Precision: 0.7385

Recall: 0.6957

F1-score: 0.7164

ROC-AUC: 0.8319

ğŸ¯ Final Model Chosen:
-----------------------------------------------------

â­ Logistic Regression â€” best performance across multiple metrics.
-------------------------------------------------------

ğŸ“ Learnings & Challenges

Handling high missingness in Cabin by extracting deck information helped preserve useful patterns.

Feature engineering (especially Title extraction and FamilySize) significantly improved model performance.

Logistic Regression performed better than Random Forest due to dataset structure and linear separability.

Maintaining consistent preprocessing between training and inference is crucial for deployment.








